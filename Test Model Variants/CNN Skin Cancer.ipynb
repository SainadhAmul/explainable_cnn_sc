{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import PIL\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten,Conv2D,MaxPool2D,BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image as imgpk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"./Dataset/HAM10000_metadata.csv\")\n",
    "metadata = metadata[['image_id', 'dx']].rename({'image_id':'imgid' , 'dx' :'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id class\n",
       "0  ISIC_0027419   bkl\n",
       "1  ISIC_0025030   bkl\n",
       "2  ISIC_0026769   bkl\n",
       "3  ISIC_0025661   bkl\n",
       "4  ISIC_0031633   bkl"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.rename({'image_id': 'image_id', 'dx': 'class'}, axis=1, inplace=True)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       6705\n",
       "mel      1113\n",
       "bkl      1099\n",
       "bcc       514\n",
       "akiec     327\n",
       "vasc      142\n",
       "df        115\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "metadata['class'] = le.fit_transform(metadata['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_map = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = metadata[metadata['class']==0]\n",
    "df2 = metadata[metadata['class']==1]\n",
    "df3 = metadata[metadata['class']==2]\n",
    "df4 = metadata[metadata['class']==3]\n",
    "df5 = metadata[metadata['class']==4]\n",
    "df6 = metadata[metadata['class']==5]\n",
    "df7 = metadata[metadata['class']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 200\n",
    "df1_balanced = resample(df1,replace = True,n_samples = samples,random_state = 2)\n",
    "df2_balanced = resample(df2,replace = True,n_samples = samples,random_state = 2)\n",
    "df3_balanced = resample(df3,replace = True,n_samples = samples,random_state = 2)\n",
    "df4_balanced = resample(df4,replace = True,n_samples = samples,random_state = 2)\n",
    "df5_balanced = resample(df5,replace = True,n_samples = samples,random_state = 2)\n",
    "df6_balanced = resample(df6,replace = True,n_samples = samples,random_state = 2)\n",
    "df7_balanced = resample(df7,replace = True,n_samples = samples,random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([df1_balanced,df2_balanced,df3_balanced,\n",
    "                  df4_balanced,df5_balanced,df6_balanced,df7_balanced])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./Dataset/Skin Cancer/Skin Cancer\"\n",
    "id = [x.split('.')[0] for x in os.listdir(dir)]\n",
    "image_path = {x:os.path.join(dir,f'{x}.jpg') for x in id}\n",
    "merged['path'] = merged['image_id'].map(image_path.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9855</th>\n",
       "      <td>ISIC_0029851</td>\n",
       "      <td>0</td>\n",
       "      <td>./Dataset/Skin Cancer/Skin Cancer\\ISIC_0029851...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  class                                               path\n",
       "9855  ISIC_0029851      0  ./Dataset/Skin Cancer/Skin Cancer\\ISIC_0029851..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN MODEL (PYTORCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, csv_path, images_folder, transform = None):\n",
    "#         self.df = pd.read_csv(csv_path)\n",
    "#         self.images_folder = images_folder\n",
    "#         self.transform = transform\n",
    "#         self.class2index = {\"cat\":0, \"dog\":1}\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "#     def __getitem__(self, index):\n",
    "#         filename = self.df[index, \"FILENAME\"]\n",
    "#         label = self.class2index[self.df[index, \"LABEL\"]]\n",
    "#         image = PIL.Image.open(os.path.join(self.images_folder, filename))\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Args:\n",
    "\n",
    "    # Data Loading\n",
    "    # ============\n",
    "\n",
    "    train_batch_size = 64\n",
    "    val_batch_size = 64\n",
    "    num_workers = 4\n",
    "\n",
    "    # Augmentation\n",
    "    # ============\n",
    "    horizontal_flip_prob = 0.2\n",
    "    vertical_flip_prob = 0.0\n",
    "    gaussian_blur_prob = 0.0\n",
    "    rotate_degree = 20\n",
    "    cutout = 0.3\n",
    "\n",
    "    # Training\n",
    "    # ========\n",
    "    random_seed = 1\n",
    "    epochs = 50\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    lr_step_size = 25\n",
    "    lr_gamma = 0.1\n",
    "\n",
    "    # Evaluation\n",
    "    # ==========\n",
    "    sample_count = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transforms:\n",
    "    \n",
    "\n",
    "    def __init__(self, train = True,  **transform_args):\n",
    "        \n",
    "        \n",
    "        ## ARGS\n",
    "        \n",
    "        horizontal_flip_prob = transform_args['horizontal_flip_prob']\n",
    "        vertical_flip_prob = transform_args['vertical_flip_prob']\n",
    "        gaussian_blur_prob = transform_args['gaussian_blur_prob']\n",
    "        rotate_degree = transform_args['rotate_degree']\n",
    "        cutout = transform_args['cutout']\n",
    "        cutout_height = transform_args['cutout_height']\n",
    "        cutout_width = transform_args['cutout_width'] \n",
    "    \n",
    "        \n",
    "        mean=(0.5, 0.5, 0.5)\n",
    "        std=(0.5, 0.5, 0.5)\n",
    "        \n",
    "        # Train phase transformations\n",
    "         \n",
    "        transforms_list = []\n",
    "    \n",
    "        if train:\n",
    "            if horizontal_flip_prob > 0:  # Horizontal Flip\n",
    "                transforms_list += [A.HorizontalFlip(p=horizontal_flip_prob)]\n",
    "            if vertical_flip_prob > 0:  # Vertical Flip\n",
    "                transforms_list += [A.VerticalFlip(p=vertical_flip_prob)]\n",
    "            if gaussian_blur_prob > 0:  # Patch Gaussian Augmentation\n",
    "                transforms_list += [A.GaussianBlur(p=gaussian_blur_prob)]\n",
    "            if rotate_degree > 0:  # Rotate image\n",
    "                transforms_list += [A.Rotate(limit=rotate_degree)]\n",
    "            if cutout > 0:  # CutOut\n",
    "                transforms_list += [A.CoarseDropout(\n",
    "                    p=cutout, max_holes=1, fill_value=tuple([x * 255.0 for x in mean]),\n",
    "                    max_height=cutout_height, max_width=cutout_width, min_height=1, min_width=1\n",
    "                )]\n",
    "          \n",
    "    \n",
    "        transforms_list += [\n",
    "            # normalize the data with mean and standard deviation to keep values in range [-1, 1]\n",
    "            # since there are 3 channels for each image,\n",
    "            # we have to specify mean and std for each channel\n",
    "            A.Normalize(mean=mean, std=std, always_apply=True),\n",
    "            \n",
    "            # convert the data to torch.FloatTensor\n",
    "            # with values within the range [0.0 ,1.0]\n",
    "            ToTensor()\n",
    "        ]\n",
    "    \n",
    "    \n",
    "        self.transform =  A.Compose(transforms_list)    \n",
    "\n",
    "    def __call__(self, image):\n",
    "        \"\"\"Process and image through the data transformation pipeline.\n",
    "\n",
    "        Args:\n",
    "            image: Image to process.\n",
    "        \n",
    "        Returns:\n",
    "            Transformed image.\n",
    "        \"\"\"\n",
    "\n",
    "        image = np.array(image)\n",
    "        image = self.transform(image=image)['image']\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_met, test_met = train_test_split(merged, test_size = 0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform = None):\n",
    "        self.df = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, index):\n",
    "#         print('INDEX : ', index)\n",
    "        image_path = self.df[['path']].iloc[index].item()\n",
    "#         print('PAth : |||', image_path)\n",
    "        image = np.asarray(imgpk.open(image_path).resize((224,224)))\n",
    "        label = self.df[['class']].iloc[index].item()\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>class</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>ISIC_0034252</td>\n",
       "      <td>2</td>\n",
       "      <td>./Dataset/Skin Cancer/Skin Cancer\\ISIC_0034252...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>ISIC_0024948</td>\n",
       "      <td>0</td>\n",
       "      <td>./Dataset/Skin Cancer/Skin Cancer\\ISIC_0024948...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id  class                                               path\n",
       "977    ISIC_0034252      2  ./Dataset/Skin Cancer/Skin Cancer\\ISIC_0034252...\n",
       "10006  ISIC_0024948      0  ./Dataset/Skin Cancer/Skin Cancer\\ISIC_0024948..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_met.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_args = {}\n",
    "\n",
    "\n",
    "transform_args['horizontal_flip_prob'] =  Args.horizontal_flip_prob\n",
    "transform_args['vertical_flip_prob'] = Args.vertical_flip_prob\n",
    "transform_args['gaussian_blur_prob'] = Args.gaussian_blur_prob\n",
    "transform_args['rotate_degree'] = Args.rotate_degree\n",
    "transform_args['cutout'] = 0.3\n",
    "transform_args['cutout_height'] = 16\n",
    "transform_args['cutout_width'] = 16\n",
    "\n",
    "\n",
    "train_transforms = Transforms(train=True, **transform_args)\n",
    "test_transforms = Transforms(train = False , **transform_args)  \n",
    "\n",
    "train_dataset = CustomDataset(train_met, train_transforms )\n",
    "test_dataset = CustomDataset(test_met, test_transforms)\n",
    "\n",
    "\n",
    "image, label = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(force_cpu = True):\n",
    "    \n",
    "    if force_cpu:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    else:\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cpu':\n",
    "    cuda = False\n",
    "else:\n",
    "    cuda = True\n",
    "\n",
    "dataloader_args = dict(shuffle=True, batch_size=Args.train_batch_size, num_workers=Args.num_workers, pin_memory=True) if cuda else dict(shuffle=True, batch_size=Args.train_batch_size)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **dataloader_args)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, label in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7333,  0.7333,  0.7333,  ...,  0.7647,  0.7569,  0.7569],\n",
       "          [ 0.7255,  0.7255,  0.7333,  ...,  0.7647,  0.7647,  0.7647],\n",
       "          [ 0.7255,  0.7333,  0.7255,  ...,  0.7647,  0.7725,  0.7725],\n",
       "          ...,\n",
       "          [ 0.7804,  0.7804,  0.7882,  ...,  0.7569,  0.7569,  0.7490],\n",
       "          [ 0.7882,  0.7882,  0.7882,  ...,  0.7647,  0.7647,  0.7647],\n",
       "          [ 0.7882,  0.7961,  0.7961,  ...,  0.7569,  0.7647,  0.7647]],\n",
       "\n",
       "         [[ 0.1137,  0.0745,  0.0745,  ...,  0.4118,  0.4039,  0.3961],\n",
       "          [ 0.0824,  0.0588,  0.0667,  ...,  0.4039,  0.4039,  0.4039],\n",
       "          [ 0.0824,  0.0824,  0.0824,  ...,  0.4118,  0.4118,  0.4039],\n",
       "          ...,\n",
       "          [ 0.3020,  0.2784,  0.2863,  ...,  0.4039,  0.4039,  0.3961],\n",
       "          [ 0.3333,  0.3098,  0.3098,  ...,  0.4118,  0.4118,  0.4118],\n",
       "          [ 0.3490,  0.3255,  0.3255,  ...,  0.4039,  0.4118,  0.4118]],\n",
       "\n",
       "         [[ 0.1059,  0.0745,  0.0902,  ...,  0.4353,  0.4275,  0.4196],\n",
       "          [ 0.1059,  0.0745,  0.0902,  ...,  0.4275,  0.4275,  0.4275],\n",
       "          [ 0.1137,  0.0980,  0.0980,  ...,  0.4353,  0.4275,  0.4196],\n",
       "          ...,\n",
       "          [ 0.2863,  0.2784,  0.2863,  ...,  0.3804,  0.3804,  0.3725],\n",
       "          [ 0.3098,  0.3098,  0.3020,  ...,  0.3804,  0.3882,  0.3882],\n",
       "          [ 0.3255,  0.3176,  0.3176,  ...,  0.3725,  0.3804,  0.3804]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7333,  0.7333,  0.7412,  ...,  0.2314,  0.2157,  0.2314],\n",
       "          [ 0.7333,  0.7255,  0.7412,  ...,  0.2314,  0.2392,  0.2235],\n",
       "          [ 0.7176,  0.7255,  0.7412,  ...,  0.2157,  0.2235,  0.2471],\n",
       "          ...,\n",
       "          [ 0.3647,  0.3490,  0.3412,  ...,  0.6235,  0.6078,  0.6157],\n",
       "          [ 0.3647,  0.3569,  0.3647,  ...,  0.6314,  0.6157,  0.6078],\n",
       "          [ 0.3725,  0.3725,  0.3804,  ...,  0.6314,  0.5922,  0.5843]],\n",
       "\n",
       "         [[ 0.1529,  0.1608,  0.1608,  ..., -0.3255, -0.3255, -0.3098],\n",
       "          [ 0.1529,  0.1451,  0.1529,  ..., -0.3255, -0.3255, -0.3098],\n",
       "          [ 0.1451,  0.1529,  0.1686,  ..., -0.3255, -0.3255, -0.3255],\n",
       "          ...,\n",
       "          [-0.1765, -0.1686, -0.1529,  ...,  0.1922,  0.1608,  0.1373],\n",
       "          [-0.1765, -0.1686, -0.1529,  ...,  0.1608,  0.1451,  0.1294],\n",
       "          [-0.1686, -0.1608, -0.1608,  ...,  0.1451,  0.1216,  0.1216]],\n",
       "\n",
       "         [[ 0.2784,  0.2784,  0.2627,  ..., -0.1608, -0.1608, -0.1529],\n",
       "          [ 0.2706,  0.2706,  0.2863,  ..., -0.1608, -0.1529, -0.1451],\n",
       "          [ 0.2627,  0.2549,  0.2784,  ..., -0.1765, -0.1451, -0.1294],\n",
       "          ...,\n",
       "          [-0.0824, -0.0824, -0.0824,  ...,  0.2078,  0.1686,  0.1686],\n",
       "          [-0.0902, -0.0745, -0.0667,  ...,  0.1765,  0.1608,  0.1686],\n",
       "          [-0.0902, -0.0745, -0.0667,  ...,  0.1765,  0.1373,  0.1529]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2549,  0.2627,  0.2471,  ...,  0.2157,  0.2078,  0.2000],\n",
       "          [ 0.2314,  0.2392,  0.2392,  ...,  0.2157,  0.2078,  0.2157],\n",
       "          [ 0.1843,  0.2157,  0.2549,  ...,  0.2000,  0.1765,  0.1922],\n",
       "          ...,\n",
       "          [ 0.1059,  0.0980,  0.0980,  ...,  0.2471,  0.2627,  0.2627],\n",
       "          [ 0.0902,  0.0824,  0.0902,  ...,  0.2471,  0.2784,  0.2863],\n",
       "          [ 0.0902,  0.1059,  0.1059,  ...,  0.2392,  0.2706,  0.2863]],\n",
       "\n",
       "         [[-0.0353, -0.0275, -0.0588,  ..., -0.0745, -0.0588, -0.0745],\n",
       "          [-0.0902, -0.0745, -0.0588,  ..., -0.0745, -0.0902, -0.0745],\n",
       "          [-0.1451, -0.1059, -0.0431,  ..., -0.0980, -0.1294, -0.1137],\n",
       "          ...,\n",
       "          [-0.2627, -0.2784, -0.2627,  ...,  0.0118,  0.0275,  0.0275],\n",
       "          [-0.2941, -0.2784, -0.2706,  ...,  0.0039,  0.0431,  0.0431],\n",
       "          [-0.3020, -0.2627, -0.2627,  ...,  0.0118,  0.0353,  0.0431]],\n",
       "\n",
       "         [[-0.1137, -0.1059, -0.1294,  ..., -0.1608, -0.1529, -0.1451],\n",
       "          [-0.1843, -0.1765, -0.1294,  ..., -0.1529, -0.1608, -0.1294],\n",
       "          [-0.2549, -0.2157, -0.1137,  ..., -0.1608, -0.1843, -0.1843],\n",
       "          ...,\n",
       "          [-0.3569, -0.4039, -0.4039,  ...,  0.0039,  0.0196, -0.0039],\n",
       "          [-0.3804, -0.3882, -0.4039,  ..., -0.0039,  0.0275,  0.0118],\n",
       "          [-0.3490, -0.3333, -0.3647,  ..., -0.0039,  0.0196,  0.0118]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.6549,  0.6706,  0.6549,  ...,  0.7333,  0.7176,  0.7020],\n",
       "          [ 0.6863,  0.7020,  0.6863,  ...,  0.7255,  0.7098,  0.7176],\n",
       "          [ 0.7020,  0.7098,  0.6941,  ...,  0.7176,  0.7098,  0.7255],\n",
       "          ...,\n",
       "          [ 0.7098,  0.7176,  0.7098,  ...,  0.7098,  0.7020,  0.6941],\n",
       "          [ 0.7020,  0.7020,  0.6941,  ...,  0.7020,  0.6941,  0.6941],\n",
       "          [ 0.6863,  0.7020,  0.6863,  ...,  0.6863,  0.7020,  0.7020]],\n",
       "\n",
       "         [[ 0.2235,  0.2314,  0.2157,  ...,  0.4196,  0.3882,  0.3412],\n",
       "          [ 0.2627,  0.2706,  0.2627,  ...,  0.4196,  0.3725,  0.3647],\n",
       "          [ 0.2706,  0.2784,  0.2627,  ...,  0.3882,  0.3569,  0.3725],\n",
       "          ...,\n",
       "          [ 0.3490,  0.3412,  0.3412,  ...,  0.3569,  0.3569,  0.3569],\n",
       "          [ 0.3412,  0.3333,  0.3333,  ...,  0.3569,  0.3569,  0.3569],\n",
       "          [ 0.3255,  0.3333,  0.3333,  ...,  0.3490,  0.3569,  0.3569]],\n",
       "\n",
       "         [[ 0.2392,  0.2471,  0.2157,  ...,  0.4667,  0.4431,  0.3412],\n",
       "          [ 0.2784,  0.3176,  0.2627,  ...,  0.4745,  0.4039,  0.3804],\n",
       "          [ 0.2941,  0.3255,  0.2941,  ...,  0.4118,  0.3804,  0.3961],\n",
       "          ...,\n",
       "          [ 0.4431,  0.4588,  0.4667,  ...,  0.4745,  0.4510,  0.4353],\n",
       "          [ 0.4275,  0.4588,  0.4588,  ...,  0.4667,  0.4510,  0.4353],\n",
       "          [ 0.4039,  0.4196,  0.4275,  ...,  0.4353,  0.4667,  0.4667]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5922,  0.6078,  0.6000,  ...,  0.5529,  0.5294,  0.5294],\n",
       "          [ 0.6000,  0.6157,  0.5922,  ...,  0.5529,  0.5373,  0.5451],\n",
       "          [ 0.5843,  0.5843,  0.5843,  ...,  0.5529,  0.5529,  0.4980],\n",
       "          ...,\n",
       "          [ 0.6392,  0.6000,  0.6157,  ...,  0.5294,  0.5451,  0.5373],\n",
       "          [ 0.6549,  0.6314,  0.6235,  ...,  0.5059,  0.4980,  0.5216],\n",
       "          [ 0.6549,  0.6549,  0.6392,  ...,  0.5137,  0.5137,  0.5216]],\n",
       "\n",
       "         [[ 0.2549,  0.2549,  0.2549,  ...,  0.1608,  0.1294,  0.1216],\n",
       "          [ 0.2314,  0.2314,  0.2471,  ...,  0.1686,  0.1451,  0.0902],\n",
       "          [ 0.2392,  0.2392,  0.2235,  ...,  0.1608,  0.1294,  0.0431],\n",
       "          ...,\n",
       "          [ 0.3020,  0.2706,  0.2784,  ...,  0.1608,  0.1529,  0.1529],\n",
       "          [ 0.2941,  0.2627,  0.3020,  ...,  0.1137,  0.1059,  0.1294],\n",
       "          [ 0.2941,  0.2784,  0.3098,  ...,  0.1216,  0.1216,  0.1294]],\n",
       "\n",
       "         [[ 0.3804,  0.3961,  0.3882,  ...,  0.2157,  0.1686,  0.1686],\n",
       "          [ 0.3647,  0.3569,  0.3569,  ...,  0.2157,  0.2235,  0.1843],\n",
       "          [ 0.3255,  0.3176,  0.3176,  ...,  0.2392,  0.2235,  0.1451],\n",
       "          ...,\n",
       "          [ 0.3804,  0.3412,  0.3490,  ...,  0.2000,  0.1765,  0.1765],\n",
       "          [ 0.3725,  0.3412,  0.3647,  ...,  0.1373,  0.1059,  0.1373],\n",
       "          [ 0.3804,  0.3647,  0.3804,  ...,  0.1294,  0.1294,  0.1451]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8353,  0.8510,  0.8667,  ...,  0.8196,  0.8275,  0.8196],\n",
       "          [ 0.8275,  0.8510,  0.8745,  ...,  0.8196,  0.8196,  0.8275],\n",
       "          [ 0.8353,  0.8510,  0.8745,  ...,  0.8275,  0.8353,  0.8353],\n",
       "          ...,\n",
       "          [ 0.7647,  0.7882,  0.7725,  ...,  0.7882,  0.7961,  0.7882],\n",
       "          [ 0.7804,  0.7961,  0.7725,  ...,  0.7961,  0.8039,  0.7882],\n",
       "          [ 0.7725,  0.7569,  0.7725,  ...,  0.8039,  0.7961,  0.7882]],\n",
       "\n",
       "         [[ 0.5137,  0.5373,  0.5686,  ...,  0.5608,  0.5608,  0.5608],\n",
       "          [ 0.5059,  0.5451,  0.5765,  ...,  0.5451,  0.5608,  0.5765],\n",
       "          [ 0.5216,  0.5529,  0.5922,  ...,  0.5373,  0.5843,  0.6000],\n",
       "          ...,\n",
       "          [ 0.4353,  0.4431,  0.4275,  ...,  0.5373,  0.5529,  0.5529],\n",
       "          [ 0.4510,  0.4588,  0.4353,  ...,  0.5451,  0.5451,  0.5529],\n",
       "          [ 0.4588,  0.4431,  0.4353,  ...,  0.5373,  0.5294,  0.5451]],\n",
       "\n",
       "         [[ 0.6627,  0.6941,  0.7255,  ...,  0.6078,  0.6392,  0.6314],\n",
       "          [ 0.6549,  0.7020,  0.7490,  ...,  0.5843,  0.6078,  0.6471],\n",
       "          [ 0.6941,  0.7333,  0.7804,  ...,  0.5686,  0.6000,  0.6314],\n",
       "          ...,\n",
       "          [ 0.5451,  0.5922,  0.5608,  ...,  0.5608,  0.5765,  0.6000],\n",
       "          [ 0.5843,  0.6078,  0.5843,  ...,  0.5922,  0.5922,  0.5843],\n",
       "          [ 0.5765,  0.5843,  0.5843,  ...,  0.5843,  0.5843,  0.5843]]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "# from model2 import ResNet18\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In deep learning context, the Receptive Field (RF) is defined as the size of the region in the input that produces the feature. Basically, it is a measure of association of an output feature (of any layer) to the input region (patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" This function instantiates all the model layers \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        dropout_rate = 0.01\n",
    "\n",
    "        self.convblock1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=8, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 224x224x3 | Output: 224x224x3 | RF: 3x3\n",
    "\n",
    "        self.convblock2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=8, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 224x224x3 | Output: 224x224x3 | RF: 5x5\n",
    "\n",
    "        self.convblock21 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 6x6x16 | Output: 6x6x10 | RF: 18x18\n",
    "        \n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Input: 224x224x3 | Output: 112x112x3 | RF: 6x6      jump = 1 -> 2\n",
    "    \n",
    "        self.convblock3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 112x112x3  | Output:  112x112x8  | RF: 10x10    (+4)  \n",
    "    \n",
    "        self.convblock4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 112x112x8    | Output:  112x112x8    | RF: 14x14  \n",
    "    \n",
    "        self.convblock5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 112x112x8    | Output:  112x112x8   | RF: 18x18   \n",
    "    \n",
    "        self.convblock51 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=8, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 112x112x8    | Output:  112x112x8   | RF: 18x18 \n",
    "        \n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Input: 112x112x8 | Output: 56x56x5 | RF: 20x20       jump = 2 -> 4\n",
    "    \n",
    "    \n",
    "        self.convblock6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 56x56x5 | Output:  56x56x8 | RF: 28x28   (+8)\n",
    "        \n",
    "        self.convblock7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 56x56x8 | Output:  56x56x16 | RF: 36x36\n",
    "        \n",
    "        self.convblock8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 56x56x16 | Output:  56x56x32 | RF: 44x44\n",
    "\n",
    "        self.convblock81 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=8, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 56x56x32 | Output:  56x56x8 | RF: 44x44\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Input: 56x56x16 | Output: 28x28x16 | RF: 48x48      jump = 4 -> 8\n",
    "\n",
    "        self.convblock9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 28x28x16 | Output: 28x28x16 | RF: 64x64         (+16)\n",
    "\n",
    "        self.convblock10 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: Input: 28x28x16 | Output: 28x28x16 | RF: 80x80\n",
    "        \n",
    "        \n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # Input: 56x56x16 | Output: 28x28x16 | RF: 88x88      jump = 8 -> 16\n",
    "        \n",
    "        self.convblock11 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 14x14x32 | Output: 14x14x32 | RF: 120x120     (+32)\n",
    "\n",
    "        self.convblock12 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 14x14x32 | Output: 14x14x64 | RF: 152x152\n",
    "\n",
    "\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)  # Input: 28x28x64 | Output: 14x14x64 | RF: 168x168      jump = 16 -> 32        \n",
    "\n",
    "        self.convblock13 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=7, kernel_size=1, padding = \"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(7),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )  # Input: 7x7x64 | Output: 7x7x7 | RF: 232x232   (+64)\n",
    "\n",
    "        \n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )  # Input: 6x6x10 | Output: 1x1x10 | RF: 28x28\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" This function defines the network structure \"\"\"\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock21(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        x = self.convblock5(x)\n",
    "        x = self.convblock51(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.convblock6(x)\n",
    "        x = self.convblock7(x)\n",
    "        x = self.convblock8(x)\n",
    "        x = self.convblock81(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.convblock9(x)\n",
    "        x = self.convblock10(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.convblock11(x)\n",
    "        x = self.convblock12(x)\n",
    "        x = self.pool5(x)\n",
    "        x = self.convblock13(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(-1, 7)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 224, 224]           1,544\n",
      "              ReLU-2          [-1, 8, 224, 224]               0\n",
      "       BatchNorm2d-3          [-1, 8, 224, 224]              16\n",
      "           Dropout-4          [-1, 8, 224, 224]               0\n",
      "            Conv2d-5         [-1, 16, 224, 224]           8,208\n",
      "              ReLU-6         [-1, 16, 224, 224]               0\n",
      "       BatchNorm2d-7         [-1, 16, 224, 224]              32\n",
      "           Dropout-8         [-1, 16, 224, 224]               0\n",
      "            Conv2d-9          [-1, 8, 224, 224]             136\n",
      "             ReLU-10          [-1, 8, 224, 224]               0\n",
      "      BatchNorm2d-11          [-1, 8, 224, 224]              16\n",
      "          Dropout-12          [-1, 8, 224, 224]               0\n",
      "        MaxPool2d-13          [-1, 8, 112, 112]               0\n",
      "           Conv2d-14         [-1, 16, 112, 112]           1,168\n",
      "             ReLU-15         [-1, 16, 112, 112]               0\n",
      "      BatchNorm2d-16         [-1, 16, 112, 112]              32\n",
      "          Dropout-17         [-1, 16, 112, 112]               0\n",
      "           Conv2d-18         [-1, 16, 112, 112]           2,320\n",
      "             ReLU-19         [-1, 16, 112, 112]               0\n",
      "      BatchNorm2d-20         [-1, 16, 112, 112]              32\n",
      "          Dropout-21         [-1, 16, 112, 112]               0\n",
      "           Conv2d-22         [-1, 32, 112, 112]           4,640\n",
      "             ReLU-23         [-1, 32, 112, 112]               0\n",
      "      BatchNorm2d-24         [-1, 32, 112, 112]              64\n",
      "          Dropout-25         [-1, 32, 112, 112]               0\n",
      "           Conv2d-26          [-1, 8, 112, 112]             264\n",
      "             ReLU-27          [-1, 8, 112, 112]               0\n",
      "      BatchNorm2d-28          [-1, 8, 112, 112]              16\n",
      "          Dropout-29          [-1, 8, 112, 112]               0\n",
      "        MaxPool2d-30            [-1, 8, 56, 56]               0\n",
      "           Conv2d-31            [-1, 8, 56, 56]             584\n",
      "             ReLU-32            [-1, 8, 56, 56]               0\n",
      "      BatchNorm2d-33            [-1, 8, 56, 56]              16\n",
      "          Dropout-34            [-1, 8, 56, 56]               0\n",
      "           Conv2d-35           [-1, 16, 56, 56]           1,168\n",
      "             ReLU-36           [-1, 16, 56, 56]               0\n",
      "      BatchNorm2d-37           [-1, 16, 56, 56]              32\n",
      "          Dropout-38           [-1, 16, 56, 56]               0\n",
      "           Conv2d-39           [-1, 32, 56, 56]           4,640\n",
      "             ReLU-40           [-1, 32, 56, 56]               0\n",
      "      BatchNorm2d-41           [-1, 32, 56, 56]              64\n",
      "          Dropout-42           [-1, 32, 56, 56]               0\n",
      "           Conv2d-43            [-1, 8, 56, 56]             264\n",
      "             ReLU-44            [-1, 8, 56, 56]               0\n",
      "      BatchNorm2d-45            [-1, 8, 56, 56]              16\n",
      "          Dropout-46            [-1, 8, 56, 56]               0\n",
      "        MaxPool2d-47            [-1, 8, 28, 28]               0\n",
      "           Conv2d-48           [-1, 16, 28, 28]           1,168\n",
      "             ReLU-49           [-1, 16, 28, 28]               0\n",
      "      BatchNorm2d-50           [-1, 16, 28, 28]              32\n",
      "          Dropout-51           [-1, 16, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           4,640\n",
      "             ReLU-53           [-1, 32, 28, 28]               0\n",
      "      BatchNorm2d-54           [-1, 32, 28, 28]              64\n",
      "          Dropout-55           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-56           [-1, 32, 14, 14]               0\n",
      "           Conv2d-57           [-1, 32, 14, 14]           9,248\n",
      "             ReLU-58           [-1, 32, 14, 14]               0\n",
      "      BatchNorm2d-59           [-1, 32, 14, 14]              64\n",
      "          Dropout-60           [-1, 32, 14, 14]               0\n",
      "           Conv2d-61           [-1, 64, 14, 14]          18,496\n",
      "             ReLU-62           [-1, 64, 14, 14]               0\n",
      "      BatchNorm2d-63           [-1, 64, 14, 14]             128\n",
      "          Dropout-64           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-65             [-1, 64, 7, 7]               0\n",
      "           Conv2d-66              [-1, 7, 7, 7]             455\n",
      "             ReLU-67              [-1, 7, 7, 7]               0\n",
      "      BatchNorm2d-68              [-1, 7, 7, 7]              14\n",
      "          Dropout-69              [-1, 7, 7, 7]               0\n",
      "AdaptiveAvgPool2d-70              [-1, 7, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 59,581\n",
      "Trainable params: 59,581\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 85.50\n",
      "Params size (MB): 0.23\n",
      "Estimated Total Size (MB): 86.30\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = ResNet18().to(device)\n",
    "model = Net().to(device)\n",
    "summary(model, input_size=( 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan  8 15:38:57 2021\n",
    "\n",
    "@author: saina\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# from model import Net\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, l1_factor =0,  **trackers):\n",
    "      \n",
    "    model.train()\n",
    "    correct_classified = 0\n",
    "    for batch_number , (x_train,y_train) in enumerate(train_loader):\n",
    "        \n",
    "        batch_number+=1\n",
    "        \n",
    "        x_train,y_train = x_train.to(device), y_train.to(device)\n",
    "\n",
    "        # print(x_train.shape)\n",
    "        pred = model.forward(x_train)\n",
    "\n",
    "        # print(pred.shape)\n",
    "        # print(y_train.shape)\n",
    "        loss = criterion(pred,y_train)\n",
    "        \n",
    "        ## L1 LOSS\n",
    "        if l1_factor > 0:  # Apply L1 regularization\n",
    "            l1_criteria = nn.L1Loss(size_average=False)\n",
    "            regularizer_loss = 0\n",
    "            for parameter in model.parameters():\n",
    "                regularizer_loss += l1_criteria(parameter, torch.zeros_like(parameter))\n",
    "            loss += l1_factor * regularizer_loss\n",
    "        \n",
    "        #pred.argmax(dim=1, keepdim=True)\n",
    "        #PyTorch .eq() function to do this, which compares the values in two tensors and if they match, returns a 1. If they don’t match, it returns a 0:\n",
    "        #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        predicted = torch.max(pred.data ,1)[1]\n",
    "        correct_classified += (predicted == y_train).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # train_loader.dataset.data / train_loader.batch_size\n",
    "        \n",
    "        if batch_number%100 == 0:\n",
    "            \n",
    "            acc = round((correct_classified.item())/(batch_number*train_loader.batch_size),5)\n",
    "            print(f'(TRAIN) batch_number: {batch_number:4} Loss : {loss:4.4} Acc : {acc:4.5}')\n",
    "    \n",
    "    acc = round((correct_classified.item())/len(train_loader.dataset),5)\n",
    "    \n",
    "    prev_acc = trackers['train_acc']\n",
    "    trackers['train_acc'] = prev_acc.append(acc)\n",
    "    \n",
    "    prev_losses = trackers['train_losses']\n",
    "    trackers['train_losses'] = prev_losses.append(loss.item())  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device, incorrect_samples, **trackers):\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        correct_classified = 0\n",
    "        for batch_number , (x_test,y_test) in enumerate(test_loader):\n",
    "        \n",
    "            x_test,y_test = x_test.to(device), y_test.to(device)\n",
    "            pred = model.forward(x_test)\n",
    "            loss = criterion(pred,y_test)\n",
    "            test_losses.append(loss)\n",
    "            \n",
    "            correct_classified += (torch.max(pred,1)[1] == y_test).sum()\n",
    "            \n",
    "            ## INCORRECT PRED SAMPLES !\n",
    "            output = pred.argmax(dim=1, keepdim=True)\n",
    "            result = output.eq(y_test.view_as(output))\n",
    "        \n",
    "            if len(incorrect_samples) < 25:\n",
    "                for i in range(test_loader.batch_size):\n",
    "                    if not list(result)[i]:\n",
    "                        incorrect_samples.append({\n",
    "                            'prediction': list(output)[i],\n",
    "                            'label': list(y_test.view_as(output))[i],\n",
    "                            'image': list(x_test)[i]\n",
    "                        }) \n",
    "    \n",
    "        avg_loss = torch.mean(torch.tensor(test_losses))\n",
    "        acc = round(correct_classified.item()/len(test_loader.dataset),5)\n",
    "        \n",
    "        prev_acc = trackers['test_acc']\n",
    "        trackers['test_acc'] = prev_acc.append(acc)\n",
    "        \n",
    "        prev_losses = trackers['test_losses']\n",
    "        trackers['test_losses'] = prev_losses.append(avg_loss.item())  \n",
    "        \n",
    "        print('(TEST) Correct_classified : ' , correct_classified.item() ,' of ' , len(test_loader.dataset))\n",
    "        print(f'(TEST) Loss : {avg_loss:4.4} Acc : {acc:4.5}')\n",
    "        print('\\n','*'*60 , '\\n')\n",
    "        \n",
    "\n",
    "     \n",
    "        \n",
    "     \n",
    "        \n",
    "        \n",
    "def run_model(model, train_loader, test_loader, epochs, device, **regularization):\n",
    "        \n",
    "    # model = Net().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    l2_factor = regularization['l2_factor']\n",
    "    l1_factor = regularization['l1_factor']\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=l2_factor)\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.15)\n",
    "    \n",
    "    ## TRACKERS\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    train_trackers = {'train_acc':train_acc,'train_losses':train_losses}\n",
    "    \n",
    "    test_acc = []\n",
    "    test_losses = []\n",
    "    test_trackers = {'test_acc':test_acc,'test_losses':test_losses}\n",
    "    \n",
    "    incorrect_samples = []\n",
    "    \n",
    "    ## Model RUN!\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f'\\nEpoch {epoch}:')\n",
    "        train(model,train_loader, criterion, optimizer, device,l1_factor =l1_factor, **train_trackers)\n",
    "        scheduler.step()\n",
    "        test(model, test_loader, criterion, device, incorrect_samples, **test_trackers)\n",
    "        \n",
    "    return model,train_trackers,test_trackers,incorrect_samples\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "(TEST) Correct_classified :  42  of  280\n",
      "(TEST) Loss : 2.062 Acc : 0.15\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 2:\n",
      "(TEST) Correct_classified :  91  of  280\n",
      "(TEST) Loss : 1.839 Acc : 0.325\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 3:\n",
      "(TEST) Correct_classified :  101  of  280\n",
      "(TEST) Loss : 1.692 Acc : 0.36071\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 4:\n",
      "(TEST) Correct_classified :  90  of  280\n",
      "(TEST) Loss : 1.691 Acc : 0.32143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 5:\n",
      "(TEST) Correct_classified :  107  of  280\n",
      "(TEST) Loss : 1.693 Acc : 0.38214\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 6:\n",
      "(TEST) Correct_classified :  108  of  280\n",
      "(TEST) Loss : 1.582 Acc : 0.38571\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 7:\n",
      "(TEST) Correct_classified :  103  of  280\n",
      "(TEST) Loss : 1.569 Acc : 0.36786\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 8:\n",
      "(TEST) Correct_classified :  105  of  280\n",
      "(TEST) Loss : 1.556 Acc : 0.375\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 9:\n",
      "(TEST) Correct_classified :  103  of  280\n",
      "(TEST) Loss : 1.582 Acc : 0.36786\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 10:\n",
      "(TEST) Correct_classified :  103  of  280\n",
      "(TEST) Loss : 1.559 Acc : 0.36786\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 11:\n",
      "(TEST) Correct_classified :  102  of  280\n",
      "(TEST) Loss : 1.558 Acc : 0.36429\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 12:\n",
      "(TEST) Correct_classified :  103  of  280\n",
      "(TEST) Loss : 1.573 Acc : 0.36786\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 13:\n",
      "(TEST) Correct_classified :  106  of  280\n",
      "(TEST) Loss : 1.578 Acc : 0.37857\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 14:\n",
      "(TEST) Correct_classified :  102  of  280\n",
      "(TEST) Loss : 1.577 Acc : 0.36429\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 15:\n",
      "(TEST) Correct_classified :  99  of  280\n",
      "(TEST) Loss : 1.544 Acc : 0.35357\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 16:\n",
      "(TEST) Correct_classified :  101  of  280\n",
      "(TEST) Loss : 1.56 Acc : 0.36071\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 17:\n",
      "(TEST) Correct_classified :  102  of  280\n",
      "(TEST) Loss : 1.543 Acc : 0.36429\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 18:\n",
      "(TEST) Correct_classified :  106  of  280\n",
      "(TEST) Loss : 1.576 Acc : 0.37857\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 19:\n",
      "(TEST) Correct_classified :  104  of  280\n",
      "(TEST) Loss : 1.571 Acc : 0.37143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 20:\n",
      "(TEST) Correct_classified :  103  of  280\n",
      "(TEST) Loss : 1.578 Acc : 0.36786\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 21:\n",
      "(TEST) Correct_classified :  106  of  280\n",
      "(TEST) Loss : 1.543 Acc : 0.37857\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 22:\n",
      "(TEST) Correct_classified :  104  of  280\n",
      "(TEST) Loss : 1.535 Acc : 0.37143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 23:\n",
      "(TEST) Correct_classified :  101  of  280\n",
      "(TEST) Loss : 1.537 Acc : 0.36071\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 24:\n",
      "(TEST) Correct_classified :  104  of  280\n",
      "(TEST) Loss : 1.548 Acc : 0.37143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 25:\n",
      "(TEST) Correct_classified :  104  of  280\n",
      "(TEST) Loss : 1.57 Acc : 0.37143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 26:\n",
      "(TEST) Correct_classified :  105  of  280\n",
      "(TEST) Loss : 1.559 Acc : 0.375\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 27:\n",
      "(TEST) Correct_classified :  104  of  280\n",
      "(TEST) Loss : 1.549 Acc : 0.37143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 28:\n",
      "(TEST) Correct_classified :  105  of  280\n",
      "(TEST) Loss : 1.527 Acc : 0.375\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 29:\n",
      "(TEST) Correct_classified :  104  of  280\n",
      "(TEST) Loss : 1.552 Acc : 0.37143\n",
      "\n",
      " ************************************************************ \n",
      "\n",
      "\n",
      "Epoch 30:\n",
      "(TEST) Correct_classified :  103  of  280\n",
      "(TEST) Loss : 1.556 Acc : 0.36786\n",
      "\n",
      " ************************************************************ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "regularization = {'l1_factor':0,'l2_factor':0}\n",
    "\n",
    "model,train_trackers,test_trackers,incorrect_samples = run_model(model, train_loader, test_loader, epochs, device, **regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() ,'C:\\\\Users\\\\saina\\\\Documents\\\\Assignements\\\\Dissertation\\\\Saved Models\\\\Custom_trained_sample.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAD CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST DATA TO TRY GRADCAM\n",
    "import shutil\n",
    "test_dir_path = '.\\\\Dataset\\\\test\\\\'\n",
    "\n",
    "for index, row in sample_data.iterrows():\n",
    "#     print(row['class'], row['c2'])\n",
    "    \n",
    "    if not os.path.exists(test_dir_path + str(row['class'])):\n",
    "        os.makedirs(test_dir_path + str(row['class']))\n",
    "        \n",
    "    shutil.copy(\".\\\\Dataset\\\\Skin Cancer\\\\Skin Cancer\\\\\"  + row['image_id'] + '.jpg', test_dir_path + str(row['class']) + '\\\\' + row['image_id'] + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGradCam(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        \n",
    "        # get the pretrained resnet network\n",
    "        self.res = model\n",
    "        \n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = nn.Sequential(*list(self.res.children())[:-2])\n",
    "        \n",
    "        # # get the max pool of the features stem\n",
    "        # self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of the resnet\n",
    "        self.classifier1 = list(self.res.children())[-2:][0]\n",
    "        self.classifier2 = list(self.res.children())[-2:][1]\n",
    "\n",
    "        \n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "    \n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "        \n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "\n",
    "        # apply the remaining pooling\n",
    "        x = self.classifier1(x)\n",
    "        x = F.avg_pool2d(x, 4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier2(x)\n",
    "        return x\n",
    "    \n",
    "    # method for the gradient extraction\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    # method for the activation exctraction\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = merged.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import Transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "test_transforms = Transforms(train=False, **transform_args)\n",
    "data = datasets.ImageFolder(\"C:\\\\Users\\\\saina\\\\Documents\\\\Assignements\\\\Dissertation\\\\Dataset\\\\test\\\\\", test_transforms)\n",
    "sample_loader = torch.utils.data.DataLoader(data, batch_size = 1, shuffle=False, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loader.dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device2 = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,label in sample_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.transpose(img.squeeze().cpu().data.numpy(), (1,2,0))\n",
    "# img1 = img1[...,::-1]\n",
    "plt.imshow( img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the resnet model\n",
    "res = ModelGradCam(model)\n",
    "# set the evaluation mode\n",
    "res.eval()\n",
    "# get the most likely prediction of the model\n",
    "pred = res(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.get_activations_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = res.get_activations_gradient()\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "# get the activations of the last convolutional layer\n",
    "activations = res.get_activations(img).detach()\n",
    "pooled_gradients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
